{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>260097528899452929</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Won the match #getin . Plus\\u002c tomorrow is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>263791921753882624</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Some areas of New England could see the first ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>264194578381410304</td>\n",
       "      <td>negative</td>\n",
       "      <td>@francesco_con40 2nd worst QB. DEFINITELY Tony...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>264041328420204544</td>\n",
       "      <td>neutral</td>\n",
       "      <td>#Thailand Washington - US President Barack Oba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>263816256640126976</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Did y\\u2019all hear what Tony Romo dressed up ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id sentiment  \\\n",
       "0  260097528899452929   neutral   \n",
       "1  263791921753882624   neutral   \n",
       "2  264194578381410304  negative   \n",
       "3  264041328420204544   neutral   \n",
       "4  263816256640126976   neutral   \n",
       "\n",
       "                                            sentence  \n",
       "0  Won the match #getin . Plus\\u002c tomorrow is ...  \n",
       "1  Some areas of New England could see the first ...  \n",
       "2  @francesco_con40 2nd worst QB. DEFINITELY Tony...  \n",
       "3  #Thailand Washington - US President Barack Oba...  \n",
       "4  Did y\\u2019all hear what Tony Romo dressed up ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_2013dev_a = pd.read_csv('dataset/twitter-2013dev-A.txt', delimiter=\"\\t\", names=[\"id\", \"sentiment\", \"sentence\"])\n",
    "twitter_2013test_a = pd.read_csv('dataset/twitter-2013test-A.txt', delimiter=\"\\t\", names=[\"id\", \"sentiment\", \"sentence\"])\n",
    "twitter_2013train_a = pd.read_csv('dataset/twitter-2013train-A.txt', delimiter=\"\\t\", names=[\"id\", \"sentiment\", \"sentence\"])\n",
    "twitter_2014sarcasm_a = pd.read_csv('dataset/twitter-2014sarcasm-A.txt', delimiter=\"\\t\", names=[\"id\", \"sentiment\", \"sentence\"])\n",
    "twitter_2014test_a = pd.read_csv('dataset/twitter-2014test-A.txt', delimiter=\"\\t\", names=[\"id\", \"sentiment\", \"sentence\"])\n",
    "twitter_2015test_a = pd.read_csv('dataset/twitter-2015test-A.txt', delimiter=\"\\t\", names=[\"id\", \"sentiment\", \"sentence\"])\n",
    "twitter_2015train_a = pd.read_csv('dataset/twitter-2015train-A.txt', delimiter=\"\\t\", names=[\"id\", \"sentiment\", \"sentence\"])\n",
    "twitter_2016dev_a = pd.read_csv('dataset/twitter-2016dev-A.txt', delimiter=\"\\t\", names=[\"id\", \"sentiment\", \"sentence\"])\n",
    "twitter_2016devtest_a = pd.read_csv('dataset/twitter-2016devtest-A.txt', delimiter=\"\\t\", names=[\"id\", \"sentiment\", \"sentence\"])\n",
    "twitter_2016test_a = pd.read_csv('dataset/twitter-2016test-A.txt', delimiter=\"\\t\", names=[\"id\", \"sentiment\", \"sentence\", \"dates\"])\n",
    "twitter_2016train_a = pd.read_csv('dataset/twitter-2016train-A.txt', delimiter=\"\\t\", names=[\"id\", \"sentiment\", \"sentence\"])\n",
    "\n",
    "data_df = pd.DataFrame()\n",
    "# data_df = pd.concat([twitter_2013dev_a, twitter_2013test_a, twitter_2013train_a, twitter_2014sarcasm_a, twitter_2014test_a,\n",
    "#                      twitter_2015test_a, twitter_2015train_a, twitter_2016dev_a, twitter_2016devtest_a,\n",
    "#                      twitter_2016test_a[[\"id\", \"sentiment\", \"sentence\"]], twitter_2016train_a])\n",
    "\n",
    "data_df = pd.concat([twitter_2013dev_a, twitter_2013test_a, twitter_2013train_a])\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for null values on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 14885 entries, 0 to 9683\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   id         14885 non-null  int64 \n",
      " 1   sentiment  14885 non-null  object\n",
      " 2   sentence   14885 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 465.2+ KB\n"
     ]
    }
   ],
   "source": [
    "data_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the ID column and reset the indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Won the match #getin . Plus\\u002c tomorrow is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Some areas of New England could see the first ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>@francesco_con40 2nd worst QB. DEFINITELY Tony...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neutral</td>\n",
       "      <td>#Thailand Washington - US President Barack Oba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Did y\\u2019all hear what Tony Romo dressed up ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                           sentence\n",
       "0   neutral  Won the match #getin . Plus\\u002c tomorrow is ...\n",
       "1   neutral  Some areas of New England could see the first ...\n",
       "2  negative  @francesco_con40 2nd worst QB. DEFINITELY Tony...\n",
       "3   neutral  #Thailand Washington - US President Barack Oba...\n",
       "4   neutral  Did y\\u2019all hear what Tony Romo dressed up ..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.drop(\"id\", axis=1, inplace=True)\n",
    "data_df.reset_index(inplace=True, drop=True)\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_irr_char(sentence):\n",
    "    \"\"\"\n",
    "        This function removes irrelevant characters such as punctuations and etc.\n",
    "        Accepts a string sentence\n",
    "        Returns a string cleaned sentence\n",
    "    \"\"\"\n",
    "    word_list = []\n",
    "    tokenized_sent = nltk.word_tokenize(sentence)\n",
    "    for i in tokenized_sent:\n",
    "        word_list.append(''.join(j for j in i if i.isalnum()))\n",
    "\n",
    "    word_list = list(filter(None,  word_list))\n",
    "\n",
    "    return ' '.join(word_list).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentence</th>\n",
       "      <th>cleaned_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Won the match #getin . Plus\\u002c tomorrow is ...</td>\n",
       "      <td>won the match getin tomorrow is a very busy wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Some areas of New England could see the first ...</td>\n",
       "      <td>some areas of new england could see the first ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>@francesco_con40 2nd worst QB. DEFINITELY Tony...</td>\n",
       "      <td>2nd worst qb definitely tony romo the man who ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neutral</td>\n",
       "      <td>#Thailand Washington - US President Barack Oba...</td>\n",
       "      <td>thailand washington us president barack obama ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Did y\\u2019all hear what Tony Romo dressed up ...</td>\n",
       "      <td>did hear what tony romo dressed up as for hall...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                           sentence  \\\n",
       "0   neutral  Won the match #getin . Plus\\u002c tomorrow is ...   \n",
       "1   neutral  Some areas of New England could see the first ...   \n",
       "2  negative  @francesco_con40 2nd worst QB. DEFINITELY Tony...   \n",
       "3   neutral  #Thailand Washington - US President Barack Oba...   \n",
       "4   neutral  Did y\\u2019all hear what Tony Romo dressed up ...   \n",
       "\n",
       "                                    cleaned_sentence  \n",
       "0  won the match getin tomorrow is a very busy wi...  \n",
       "1  some areas of new england could see the first ...  \n",
       "2  2nd worst qb definitely tony romo the man who ...  \n",
       "3  thailand washington us president barack obama ...  \n",
       "4  did hear what tony romo dressed up as for hall...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df[\"cleaned_sentence\"] = data_df.apply(lambda x: remove_irr_char(x[1]), axis=1)\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(sentence):\n",
    "    \"\"\"\n",
    "        This function removes stop words from the cleaned sentence\n",
    "        Accepts a string\n",
    "        Returns a string without the stop words\n",
    "    \"\"\"\n",
    "    tokenized = nltk.word_tokenize(sentence)\n",
    "    stop_words = set({\"a\", \"about\", \"an\", \"are\", \"as\", \"at\", \"be\", \"by\", \"com\", \"de\", \"en\", \"for\", \"from\", \"how\", \"i\", \"in\",\n",
    "                      \"is\", \"it\", \"la\", \"of\", \"on\", \"or\", \"that\", \"this\", \"to\", \"was\", \"what\", \"when\", \"where\", \"who\", \"will\",\n",
    "                      \"with\", \"und\", \"the\", \"www\"})\n",
    "\n",
    "    filtered_list = [\n",
    "        word for word in tokenized if word.casefold() not in stop_words\n",
    "    ]\n",
    "\n",
    "    return ' '.join(filtered_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentence</th>\n",
       "      <th>cleaned_sentence</th>\n",
       "      <th>stopwords_remove_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Won the match #getin . Plus\\u002c tomorrow is ...</td>\n",
       "      <td>won the match getin tomorrow is a very busy wi...</td>\n",
       "      <td>won match getin tomorrow very busy awareness a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Some areas of New England could see the first ...</td>\n",
       "      <td>some areas of new england could see the first ...</td>\n",
       "      <td>some areas new england could see first flakes ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>@francesco_con40 2nd worst QB. DEFINITELY Tony...</td>\n",
       "      <td>2nd worst qb definitely tony romo the man who ...</td>\n",
       "      <td>2nd worst qb definitely tony romo man likes sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neutral</td>\n",
       "      <td>#Thailand Washington - US President Barack Oba...</td>\n",
       "      <td>thailand washington us president barack obama ...</td>\n",
       "      <td>thailand washington us president barack obama ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Did y\\u2019all hear what Tony Romo dressed up ...</td>\n",
       "      <td>did hear what tony romo dressed up as for hall...</td>\n",
       "      <td>did hear tony romo dressed up halloween giants...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                           sentence  \\\n",
       "0   neutral  Won the match #getin . Plus\\u002c tomorrow is ...   \n",
       "1   neutral  Some areas of New England could see the first ...   \n",
       "2  negative  @francesco_con40 2nd worst QB. DEFINITELY Tony...   \n",
       "3   neutral  #Thailand Washington - US President Barack Oba...   \n",
       "4   neutral  Did y\\u2019all hear what Tony Romo dressed up ...   \n",
       "\n",
       "                                    cleaned_sentence  \\\n",
       "0  won the match getin tomorrow is a very busy wi...   \n",
       "1  some areas of new england could see the first ...   \n",
       "2  2nd worst qb definitely tony romo the man who ...   \n",
       "3  thailand washington us president barack obama ...   \n",
       "4  did hear what tony romo dressed up as for hall...   \n",
       "\n",
       "                           stopwords_remove_sentence  \n",
       "0  won match getin tomorrow very busy awareness a...  \n",
       "1  some areas new england could see first flakes ...  \n",
       "2  2nd worst qb definitely tony romo man likes sh...  \n",
       "3  thailand washington us president barack obama ...  \n",
       "4  did hear tony romo dressed up halloween giants...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df[\"stopwords_remove_sentence\"] = data_df.apply(lambda x: remove_stop_words(x[2]), axis=1)\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_tokens(sentence):\n",
    "    \"\"\"\n",
    "        This function stems the sentence to convert each tokens to their root base\n",
    "        Accepts a string\n",
    "        Returns a string sentence in their root base\n",
    "    \"\"\"\n",
    "    word_list = []\n",
    "    tokenized_sent = nltk.word_tokenize(sentence)\n",
    "    for i in tokenized_sent:\n",
    "        stem_word = stemmer.stem(i)\n",
    "        word_list.append(stem_word)\n",
    "\n",
    "    return ' '.join(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentence</th>\n",
       "      <th>cleaned_sentence</th>\n",
       "      <th>stopwords_remove_sentence</th>\n",
       "      <th>stemmed_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Won the match #getin . Plus\\u002c tomorrow is ...</td>\n",
       "      <td>won the match getin tomorrow is a very busy wi...</td>\n",
       "      <td>won match getin tomorrow very busy awareness a...</td>\n",
       "      <td>won match getin tomorrow veri busi awar and de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Some areas of New England could see the first ...</td>\n",
       "      <td>some areas of new england could see the first ...</td>\n",
       "      <td>some areas new england could see first flakes ...</td>\n",
       "      <td>some area new england could see first flake se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>@francesco_con40 2nd worst QB. DEFINITELY Tony...</td>\n",
       "      <td>2nd worst qb definitely tony romo the man who ...</td>\n",
       "      <td>2nd worst qb definitely tony romo man likes sh...</td>\n",
       "      <td>2nd worst qb definit toni romo man like share ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neutral</td>\n",
       "      <td>#Thailand Washington - US President Barack Oba...</td>\n",
       "      <td>thailand washington us president barack obama ...</td>\n",
       "      <td>thailand washington us president barack obama ...</td>\n",
       "      <td>thailand washington us presid barack obama vow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Did y\\u2019all hear what Tony Romo dressed up ...</td>\n",
       "      <td>did hear what tony romo dressed up as for hall...</td>\n",
       "      <td>did hear tony romo dressed up halloween giants...</td>\n",
       "      <td>did hear toni romo dress up halloween giant qu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                           sentence  \\\n",
       "0   neutral  Won the match #getin . Plus\\u002c tomorrow is ...   \n",
       "1   neutral  Some areas of New England could see the first ...   \n",
       "2  negative  @francesco_con40 2nd worst QB. DEFINITELY Tony...   \n",
       "3   neutral  #Thailand Washington - US President Barack Oba...   \n",
       "4   neutral  Did y\\u2019all hear what Tony Romo dressed up ...   \n",
       "\n",
       "                                    cleaned_sentence  \\\n",
       "0  won the match getin tomorrow is a very busy wi...   \n",
       "1  some areas of new england could see the first ...   \n",
       "2  2nd worst qb definitely tony romo the man who ...   \n",
       "3  thailand washington us president barack obama ...   \n",
       "4  did hear what tony romo dressed up as for hall...   \n",
       "\n",
       "                           stopwords_remove_sentence  \\\n",
       "0  won match getin tomorrow very busy awareness a...   \n",
       "1  some areas new england could see first flakes ...   \n",
       "2  2nd worst qb definitely tony romo man likes sh...   \n",
       "3  thailand washington us president barack obama ...   \n",
       "4  did hear tony romo dressed up halloween giants...   \n",
       "\n",
       "                                    stemmed_sentence  \n",
       "0  won match getin tomorrow veri busi awar and de...  \n",
       "1  some area new england could see first flake se...  \n",
       "2  2nd worst qb definit toni romo man like share ...  \n",
       "3  thailand washington us presid barack obama vow...  \n",
       "4  did hear toni romo dress up halloween giant qu...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df[\"stemmed_sentence\"] = data_df.apply(lambda x: stem_tokens(x[3]), axis=1)\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check for the value distribution of the sentiment in our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "neutral     6838\n",
       "positive    5690\n",
       "negative    2357\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df[\"sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us convert the sentiment column to numerical (1 = Positive, 0 = Neutral, -1 = Negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df[\"sentiment\"] = np.where((data_df[\"sentiment\"] == \"positive\"), +1, data_df[\"sentiment\"])\n",
    "data_df[\"sentiment\"] = np.where((data_df[\"sentiment\"] == \"negative\"), -1, data_df[\"sentiment\"])\n",
    "data_df[\"sentiment\"] = np.where((data_df[\"sentiment\"] == \"neutral\"), 0, data_df[\"sentiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "0     6838\n",
       "1     5690\n",
       "-1    2357\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df[\"sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentence</th>\n",
       "      <th>cleaned_sentence</th>\n",
       "      <th>stopwords_remove_sentence</th>\n",
       "      <th>stemmed_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Won the match #getin . Plus\\u002c tomorrow is ...</td>\n",
       "      <td>won the match getin tomorrow is a very busy wi...</td>\n",
       "      <td>won match getin tomorrow very busy awareness a...</td>\n",
       "      <td>won match getin tomorrow veri busi awar and de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Some areas of New England could see the first ...</td>\n",
       "      <td>some areas of new england could see the first ...</td>\n",
       "      <td>some areas new england could see first flakes ...</td>\n",
       "      <td>some area new england could see first flake se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "      <td>@francesco_con40 2nd worst QB. DEFINITELY Tony...</td>\n",
       "      <td>2nd worst qb definitely tony romo the man who ...</td>\n",
       "      <td>2nd worst qb definitely tony romo man likes sh...</td>\n",
       "      <td>2nd worst qb definit toni romo man like share ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>#Thailand Washington - US President Barack Oba...</td>\n",
       "      <td>thailand washington us president barack obama ...</td>\n",
       "      <td>thailand washington us president barack obama ...</td>\n",
       "      <td>thailand washington us presid barack obama vow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Did y\\u2019all hear what Tony Romo dressed up ...</td>\n",
       "      <td>did hear what tony romo dressed up as for hall...</td>\n",
       "      <td>did hear tony romo dressed up halloween giants...</td>\n",
       "      <td>did hear toni romo dress up halloween giant qu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                           sentence  \\\n",
       "0         0  Won the match #getin . Plus\\u002c tomorrow is ...   \n",
       "1         0  Some areas of New England could see the first ...   \n",
       "2        -1  @francesco_con40 2nd worst QB. DEFINITELY Tony...   \n",
       "3         0  #Thailand Washington - US President Barack Oba...   \n",
       "4         0  Did y\\u2019all hear what Tony Romo dressed up ...   \n",
       "\n",
       "                                    cleaned_sentence  \\\n",
       "0  won the match getin tomorrow is a very busy wi...   \n",
       "1  some areas of new england could see the first ...   \n",
       "2  2nd worst qb definitely tony romo the man who ...   \n",
       "3  thailand washington us president barack obama ...   \n",
       "4  did hear what tony romo dressed up as for hall...   \n",
       "\n",
       "                           stopwords_remove_sentence  \\\n",
       "0  won match getin tomorrow very busy awareness a...   \n",
       "1  some areas new england could see first flakes ...   \n",
       "2  2nd worst qb definitely tony romo man likes sh...   \n",
       "3  thailand washington us president barack obama ...   \n",
       "4  did hear tony romo dressed up halloween giants...   \n",
       "\n",
       "                                    stemmed_sentence  \n",
       "0  won match getin tomorrow veri busi awar and de...  \n",
       "1  some area new england could see first flake se...  \n",
       "2  2nd worst qb definit toni romo man like share ...  \n",
       "3  thailand washington us presid barack obama vow...  \n",
       "4  did hear toni romo dress up halloween giant qu...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Model without feature engineering using  CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>cleaned_sentence</th>\n",
       "      <th>stopwords_remove_sentence</th>\n",
       "      <th>stemmed_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>won the match getin tomorrow is a very busy wi...</td>\n",
       "      <td>won match getin tomorrow very busy awareness a...</td>\n",
       "      <td>won match getin tomorrow veri busi awar and de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>some areas of new england could see the first ...</td>\n",
       "      <td>some areas new england could see first flakes ...</td>\n",
       "      <td>some area new england could see first flake se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "      <td>2nd worst qb definitely tony romo the man who ...</td>\n",
       "      <td>2nd worst qb definitely tony romo man likes sh...</td>\n",
       "      <td>2nd worst qb definit toni romo man like share ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>thailand washington us president barack obama ...</td>\n",
       "      <td>thailand washington us president barack obama ...</td>\n",
       "      <td>thailand washington us presid barack obama vow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>did hear what tony romo dressed up as for hall...</td>\n",
       "      <td>did hear tony romo dressed up halloween giants...</td>\n",
       "      <td>did hear toni romo dress up halloween giant qu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                   cleaned_sentence  \\\n",
       "0         0  won the match getin tomorrow is a very busy wi...   \n",
       "1         0  some areas of new england could see the first ...   \n",
       "2        -1  2nd worst qb definitely tony romo the man who ...   \n",
       "3         0  thailand washington us president barack obama ...   \n",
       "4         0  did hear what tony romo dressed up as for hall...   \n",
       "\n",
       "                           stopwords_remove_sentence  \\\n",
       "0  won match getin tomorrow very busy awareness a...   \n",
       "1  some areas new england could see first flakes ...   \n",
       "2  2nd worst qb definitely tony romo man likes sh...   \n",
       "3  thailand washington us president barack obama ...   \n",
       "4  did hear tony romo dressed up halloween giants...   \n",
       "\n",
       "                                    stemmed_sentence  \n",
       "0  won match getin tomorrow veri busi awar and de...  \n",
       "1  some area new england could see first flake se...  \n",
       "2  2nd worst qb definit toni romo man like share ...  \n",
       "3  thailand washington us presid barack obama vow...  \n",
       "4  did hear toni romo dress up halloween giant qu...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_mod = data_df[[\"sentiment\", \"cleaned_sentence\", \"stopwords_remove_sentence\", \"stemmed_sentence\"]]\n",
    "data_mod.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a bag of words using count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_count = CountVectorizer(\n",
    "    min_df = 3,\n",
    "    ngram_range = (1, 1)\n",
    ")\n",
    "\n",
    "vectorizer_count2 = CountVectorizer(\n",
    "    min_df = 3,\n",
    "    ngram_range = (1, 1)\n",
    ")\n",
    "\n",
    "vectorizer_count3 = CountVectorizer(\n",
    "    min_df = 3,\n",
    "    ngram_range = (1, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using stemmed sentence\n",
    "features_stemmed = vectorizer_count.fit_transform(\n",
    "    data_mod[\"stemmed_sentence\"]\n",
    ")\n",
    "\n",
    "features_stemmed_nd = features_stemmed.toarray()\n",
    "\n",
    "features_stopwords = vectorizer_count2.fit_transform(\n",
    "    data_mod[\"stopwords_remove_sentence\"]\n",
    ")\n",
    "\n",
    "features_stopwords_nd = features_stopwords.toarray()\n",
    "\n",
    "features_cleaned = vectorizer_count3.fit_transform(\n",
    "    data_mod[\"cleaned_sentence\"]\n",
    ")\n",
    "\n",
    "features_cleaned_nd = features_cleaned.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_labels = []\n",
    "for i in data_mod[\"sentiment\"]:\n",
    "    data_labels.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_stemmed_cv, X_test_stemmed_cv, y_train_stemmed_cv, y_test_stemmed_cv  = train_test_split(\n",
    "        features_stemmed_nd, \n",
    "        data_labels,\n",
    "        train_size=0.80, \n",
    "        random_state=10)\n",
    "\n",
    "X_train_stopwords_cv, X_test_stopwords_cv, y_train_stopwords_cv, y_test_stopwords_cv = train_test_split(\n",
    "    features_stopwords_nd,\n",
    "    data_labels,\n",
    "    train_size=0.80,\n",
    "    random_state=10\n",
    ")\n",
    "\n",
    "X_train_cleaned_cv, X_test_cleaned_cv, y_train_cleaned_cv, y_test_cleaned_cv = train_test_split(\n",
    "    features_cleaned_nd,\n",
    "    data_labels,\n",
    "    train_size=0.80,\n",
    "    random_state=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_model_1 = LogisticRegression(max_iter=1000)\n",
    "log_model_2 = LogisticRegression(max_iter=1000)\n",
    "log_model_3 = LogisticRegression(max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_model_1 = log_model_1.fit(X_train_stemmed_cv, y_train_stemmed_cv)\n",
    "log_model_2 = log_model_2.fit(X_train_stopwords_cv, y_train_stopwords_cv)\n",
    "log_model_3 = log_model_3.fit(X_train_cleaned_cv, y_train_cleaned_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_cv1 = log_model_1.predict(X_test_stemmed_cv)\n",
    "y_pred_cv2 = log_model_2.predict(X_test_stopwords_cv)\n",
    "y_pred_cv3 = log_model_3.predict(X_test_cleaned_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6741686261336917\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.61      0.45      0.51       483\n",
      "           0       0.68      0.76      0.72      1361\n",
      "           1       0.68      0.67      0.68      1133\n",
      "\n",
      "    accuracy                           0.67      2977\n",
      "   macro avg       0.66      0.62      0.64      2977\n",
      "weighted avg       0.67      0.67      0.67      2977\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model 1\n",
    "print(accuracy_score(y_test_stemmed_cv, y_pred_cv1))\n",
    "print(classification_report(y_test_stemmed_cv, y_pred_cv1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6627477326167283\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.57      0.42      0.49       483\n",
      "           0       0.67      0.75      0.71      1361\n",
      "           1       0.69      0.66      0.67      1133\n",
      "\n",
      "    accuracy                           0.66      2977\n",
      "   macro avg       0.64      0.61      0.62      2977\n",
      "weighted avg       0.66      0.66      0.66      2977\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model 2\n",
    "print(accuracy_score(y_test_stopwords_cv, y_pred_cv2))\n",
    "print(classification_report(y_test_stopwords_cv, y_pred_cv2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6677863621095063\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.60      0.43      0.50       483\n",
      "           0       0.67      0.75      0.71      1361\n",
      "           1       0.69      0.67      0.68      1133\n",
      "\n",
      "    accuracy                           0.67      2977\n",
      "   macro avg       0.65      0.62      0.63      2977\n",
      "weighted avg       0.66      0.67      0.66      2977\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model 3\n",
    "print(accuracy_score(y_test_cleaned_cv, y_pred_cv3))\n",
    "print(classification_report(y_test_cleaned_cv, y_pred_cv3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Model without feature engineering using TFIDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_tfidf = TfidfVectorizer(min_df=5)\n",
    "\n",
    "vectorizer_tfidf2 = TfidfVectorizer(min_df=5)\n",
    "\n",
    "vectorizer_tfidf3 = TfidfVectorizer(min_df=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_stemmed_tfidf = vectorizer_tfidf.fit_transform(data_mod[\"stemmed_sentence\"])\n",
    "\n",
    "features_stemmed_tfidf_nd = features_stemmed_tfidf.toarray()\n",
    "\n",
    "features_stopwords_tfidf = vectorizer_tfidf2.fit_transform(data_mod[\"stopwords_remove_sentence\"])\n",
    "\n",
    "features_stopwords_tfidf_nd = features_stopwords_tfidf.toarray()\n",
    "\n",
    "features_cleaned_tfidf = vectorizer_tfidf3.fit_transform(data_mod[\"cleaned_sentence\"])\n",
    "\n",
    "features_cleaned_tfidf_nd = features_cleaned_tfidf.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_stemmed_tfidf, X_test_stemmed_tfidf, y_train_stemmed_tfidf, y_test_stemmed_tfidf  = train_test_split(\n",
    "        features_stemmed_tfidf_nd, \n",
    "        data_labels,\n",
    "        train_size=0.80, \n",
    "        random_state=10)\n",
    "\n",
    "X_train_stopwords_tfidf, X_test_stopwords_tfidf, y_train_stopwords_tfidf, y_test_stopwords_tfidf  = train_test_split(\n",
    "        features_stopwords_tfidf_nd, \n",
    "        data_labels,\n",
    "        train_size=0.80, \n",
    "        random_state=10)\n",
    "\n",
    "X_train_cleaned_tfidf, X_test_cleaned_tfidf, y_train_cleaned_tfidf, y_test_cleaned_tfidf  = train_test_split(\n",
    "        features_cleaned_tfidf_nd, \n",
    "        data_labels,\n",
    "        train_size=0.80, \n",
    "        random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_model_4 =  LogisticRegression()\n",
    "log_model_5 =  LogisticRegression()\n",
    "log_model_6 =  LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Adrian Jerez\\Desktop\\NLP_Project\\NLP_Project\\project_env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Adrian Jerez\\Desktop\\NLP_Project\\NLP_Project\\project_env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Adrian Jerez\\Desktop\\NLP_Project\\NLP_Project\\project_env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "log_model_4 = log_model_4.fit(X_train_stemmed_tfidf, y_train_stemmed_tfidf)\n",
    "log_model_5 = log_model_5.fit(X_train_stopwords_tfidf, y_train_stopwords_tfidf)\n",
    "log_model_6 = log_model_6.fit(X_train_cleaned_tfidf, y_train_cleaned_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6751763520322472\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.70      0.34      0.45       483\n",
      "           0       0.66      0.81      0.73      1361\n",
      "           1       0.70      0.65      0.68      1133\n",
      "\n",
      "    accuracy                           0.68      2977\n",
      "   macro avg       0.68      0.60      0.62      2977\n",
      "weighted avg       0.68      0.68      0.66      2977\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_stemmed_tfidf = log_model_4.predict(X_test_stemmed_tfidf)\n",
    "print(accuracy_score(y_test_stemmed_tfidf, y_pred_stemmed_tfidf))\n",
    "print(classification_report(y_test_stemmed_tfidf, y_pred_stemmed_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6738327175008397\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.68      0.31      0.43       483\n",
      "           0       0.65      0.82      0.72      1361\n",
      "           1       0.71      0.65      0.68      1133\n",
      "\n",
      "    accuracy                           0.67      2977\n",
      "   macro avg       0.68      0.59      0.61      2977\n",
      "weighted avg       0.68      0.67      0.66      2977\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_stopwords_tfidf = log_model_5.predict(X_test_stopwords_tfidf)\n",
    "print(accuracy_score(y_test_stopwords_tfidf, y_pred_stopwords_tfidf))\n",
    "print(classification_report(y_test_stopwords_tfidf, y_pred_stopwords_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6731609002351361\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.69      0.33      0.44       483\n",
      "           0       0.65      0.82      0.72      1361\n",
      "           1       0.71      0.65      0.68      1133\n",
      "\n",
      "    accuracy                           0.67      2977\n",
      "   macro avg       0.68      0.60      0.61      2977\n",
      "weighted avg       0.68      0.67      0.66      2977\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_cleaned_tfidf = log_model_6.predict(X_test_cleaned_tfidf)\n",
    "print(accuracy_score(y_test_cleaned_tfidf, y_pred_cleaned_tfidf))\n",
    "print(classification_report(y_test_cleaned_tfidf, y_pred_cleaned_tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Model without Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB(force_alpha=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB(force_alpha=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB(force_alpha=True)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multinomialNB = MultinomialNB(force_alpha=True)\n",
    "\n",
    "multinomialNB.fit(X_train_stopwords_cv, y_train_stopwords_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6341954988243198\n"
     ]
    }
   ],
   "source": [
    "y_pred_nb_cv = multinomialNB.predict(X_test_stopwords_cv)\n",
    "print(accuracy_score(y_test_stopwords_cv, y_pred_nb_cv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6234464225730602\n"
     ]
    }
   ],
   "source": [
    "multinomialNB2 = MultinomialNB(force_alpha=True)\n",
    "\n",
    "multinomialNB2.fit(X_train_stopwords_tfidf, y_train_stopwords_tfidf)\n",
    "\n",
    "y_pred_nb_tfidf = multinomialNB2.predict(X_test_stopwords_tfidf)\n",
    "print(accuracy_score(y_test_stopwords_tfidf, y_pred_nb_tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Models with Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 6838, -1: 6838, 1: 6838})\n"
     ]
    }
   ],
   "source": [
    "oversample = RandomOverSampler(sampling_strategy='minority')\n",
    "\n",
    "X_over, y_over = oversample.fit_resample(features_stopwords_nd, data_labels)\n",
    "X_over, y_over = oversample.fit_resample(X_over, y_over)\n",
    "print(Counter(y_over))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_os, X_test_os, y_train_os, y_test_os  = train_test_split(\n",
    "        X_over, \n",
    "        y_over,\n",
    "        train_size=0.80, \n",
    "        random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_model_7 = LogisticRegression(max_iter=1000)\n",
    "log_model_7 = log_model_7.fit(X_train_os, y_train_os)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7652936875456983\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.81      0.89      0.85      1411\n",
      "           0       0.70      0.68      0.69      1318\n",
      "           1       0.78      0.71      0.74      1374\n",
      "\n",
      "    accuracy                           0.77      4103\n",
      "   macro avg       0.76      0.76      0.76      4103\n",
      "weighted avg       0.76      0.77      0.76      4103\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = log_model_7.predict(X_test_os)\n",
    "print(accuracy_score(y_test_os, y_pred))\n",
    "print(classification_report(y_test_os, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets apply GridSearch to get the best hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# parameters = {\n",
    "#     'C': np.logspace(-5, 8, 15),\n",
    "#     'max_iter': [1,2,3,4,5,10,20,30,40,50,60,70,80,90,100,110,120,130,140,150]\n",
    "# }\n",
    "\n",
    "# grid_search = LogisticRegression()\n",
    "\n",
    "# grid_search = GridSearchCV(\n",
    "#     grid_search,\n",
    "#     parameters,\n",
    "#     cv = 5,\n",
    "#     scoring = 'accuracy',\n",
    "#     n_jobs=-1\n",
    "# )\n",
    "\n",
    "# grid_result = grid_search.fit(X_train, y_train)\n",
    "\n",
    "# print(\"Best parameters: \", grid_result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Adrian Jerez\\Desktop\\NLP_Project\\NLP_Project\\project_env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=3.727593720314938, max_iter=40)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=3.727593720314938, max_iter=40)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(C=3.727593720314938, max_iter=40)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_model_8 = LogisticRegression(C = 3.727593720314938, max_iter=40)\n",
    "log_model_8.fit(X_train_os, y_train_os)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7567633438947112\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.80      0.87      0.84      1411\n",
      "           0       0.68      0.69      0.69      1318\n",
      "           1       0.78      0.70      0.74      1374\n",
      "\n",
      "    accuracy                           0.76      4103\n",
      "   macro avg       0.75      0.75      0.75      4103\n",
      "weighted avg       0.76      0.76      0.76      4103\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = log_model_8.predict(X_test_os)\n",
    "print(accuracy_score(y_test_os, y_pred))\n",
    "print(classification_report(y_test_os, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes with Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB(force_alpha=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB(force_alpha=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB(force_alpha=True)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multinomialNB_os = MultinomialNB(force_alpha=True)\n",
    "\n",
    "multinomialNB_os.fit(X_train_os, y_train_os)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7033877650499635\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.75      0.82      0.78      1411\n",
      "           0       0.66      0.58      0.62      1318\n",
      "           1       0.69      0.70      0.70      1374\n",
      "\n",
      "    accuracy                           0.70      4103\n",
      "   macro avg       0.70      0.70      0.70      4103\n",
      "weighted avg       0.70      0.70      0.70      4103\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_nb_os = multinomialNB_os.predict(X_test_os)\n",
    "print(accuracy_score(y_test_os, y_pred_nb_os))\n",
    "print(classification_report(y_test_os, y_pred_nb_os))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models using cleaned sentence and nltk.stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "vectorizer_tfidf = TfidfVectorizer(min_df=5, max_df=0.8, max_features=2500, stop_words = stopwords.words('english'))\n",
    "\n",
    "features_cleaned_tfidf_2 = vectorizer_tfidf.fit_transform(data_mod[\"cleaned_sentence\"])\n",
    "\n",
    "features_cleaned_tfidf_2_nd = features_cleaned_tfidf_2.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 6838, -1: 6838, 1: 6838})\n"
     ]
    }
   ],
   "source": [
    "X_over, y_over = oversample.fit_resample(features_cleaned_tfidf_2_nd, data_labels)\n",
    "X_over, y_over = oversample.fit_resample(X_over, y_over)\n",
    "print(Counter(y_over))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cleaned_tfidf_2, X_test_cleaned_tfidf_2, y_train_cleaned_tfidf_2, y_test_cleaned_tfidf_2  = train_test_split(\n",
    "        X_over, \n",
    "        y_over,\n",
    "        train_size=0.80, \n",
    "        random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_model_9 = LogisticRegression(max_iter=1000)\n",
    "log_model_9 = log_model_9.fit(X_train_cleaned_tfidf_2, y_train_cleaned_tfidf_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7007067999025104\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.73      0.76      0.75      1411\n",
      "           0       0.63      0.68      0.66      1318\n",
      "           1       0.75      0.66      0.70      1374\n",
      "\n",
      "    accuracy                           0.70      4103\n",
      "   macro avg       0.70      0.70      0.70      4103\n",
      "weighted avg       0.70      0.70      0.70      4103\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = log_model_9.predict(X_test_cleaned_tfidf_2)\n",
    "print(accuracy_score(y_test_cleaned_tfidf_2, y_pred))\n",
    "print(classification_report(y_test_cleaned_tfidf_2, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving models to a File Format using JobLib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nlp_project/models/count_vectorizer.joblib']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_filename = \"log_reg_oversampling_stopwords_78acc.joblib\"\n",
    "vectorizer_filename = \"count_vectorizer.joblib\"\n",
    "joblib.dump(log_model_7, \"nlp_project/models/\" + model_filename)\n",
    "joblib.dump(vectorizer_count2, \"nlp_project/models/\" + vectorizer_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
