{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>260097528899452929</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Won the match #getin . Plus\\u002c tomorrow is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>263791921753882624</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Some areas of New England could see the first ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>264194578381410304</td>\n",
       "      <td>negative</td>\n",
       "      <td>@francesco_con40 2nd worst QB. DEFINITELY Tony...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>264041328420204544</td>\n",
       "      <td>neutral</td>\n",
       "      <td>#Thailand Washington - US President Barack Oba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>263816256640126976</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Did y\\u2019all hear what Tony Romo dressed up ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id sentiment  \\\n",
       "0  260097528899452929   neutral   \n",
       "1  263791921753882624   neutral   \n",
       "2  264194578381410304  negative   \n",
       "3  264041328420204544   neutral   \n",
       "4  263816256640126976   neutral   \n",
       "\n",
       "                                            sentence  \n",
       "0  Won the match #getin . Plus\\u002c tomorrow is ...  \n",
       "1  Some areas of New England could see the first ...  \n",
       "2  @francesco_con40 2nd worst QB. DEFINITELY Tony...  \n",
       "3  #Thailand Washington - US President Barack Oba...  \n",
       "4  Did y\\u2019all hear what Tony Romo dressed up ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_2013dev_a = pd.read_csv('dataset/twitter-2013dev-A.txt', delimiter=\"\\t\", names=[\"id\", \"sentiment\", \"sentence\"])\n",
    "twitter_2013test_a = pd.read_csv('dataset/twitter-2013test-A.txt', delimiter=\"\\t\", names=[\"id\", \"sentiment\", \"sentence\"])\n",
    "twitter_2013train_a = pd.read_csv('dataset/twitter-2013train-A.txt', delimiter=\"\\t\", names=[\"id\", \"sentiment\", \"sentence\"])\n",
    "twitter_2014sarcasm_a = pd.read_csv('dataset/twitter-2014sarcasm-A.txt', delimiter=\"\\t\", names=[\"id\", \"sentiment\", \"sentence\"])\n",
    "twitter_2014test_a = pd.read_csv('dataset/twitter-2014test-A.txt', delimiter=\"\\t\", names=[\"id\", \"sentiment\", \"sentence\"])\n",
    "twitter_2015test_a = pd.read_csv('dataset/twitter-2015test-A.txt', delimiter=\"\\t\", names=[\"id\", \"sentiment\", \"sentence\"])\n",
    "twitter_2015train_a = pd.read_csv('dataset/twitter-2015train-A.txt', delimiter=\"\\t\", names=[\"id\", \"sentiment\", \"sentence\"])\n",
    "twitter_2016dev_a = pd.read_csv('dataset/twitter-2016dev-A.txt', delimiter=\"\\t\", names=[\"id\", \"sentiment\", \"sentence\"])\n",
    "twitter_2016devtest_a = pd.read_csv('dataset/twitter-2016devtest-A.txt', delimiter=\"\\t\", names=[\"id\", \"sentiment\", \"sentence\"])\n",
    "twitter_2016test_a = pd.read_csv('dataset/twitter-2016test-A.txt', delimiter=\"\\t\", names=[\"id\", \"sentiment\", \"sentence\", \"dates\"])\n",
    "twitter_2016train_a = pd.read_csv('dataset/twitter-2016train-A.txt', delimiter=\"\\t\", names=[\"id\", \"sentiment\", \"sentence\"])\n",
    "\n",
    "data_df = pd.DataFrame()\n",
    "# data_df = pd.concat([twitter_2013dev_a, twitter_2013test_a, twitter_2013train_a, twitter_2014sarcasm_a, twitter_2014test_a,\n",
    "#                      twitter_2015test_a, twitter_2015train_a, twitter_2016dev_a, twitter_2016devtest_a,\n",
    "#                      twitter_2016test_a[[\"id\", \"sentiment\", \"sentence\"]], twitter_2016train_a])\n",
    "\n",
    "data_df = pd.concat([twitter_2013dev_a, twitter_2013test_a, twitter_2013train_a])\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for null values on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 14885 entries, 0 to 9683\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   id         14885 non-null  int64 \n",
      " 1   sentiment  14885 non-null  object\n",
      " 2   sentence   14885 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 465.2+ KB\n"
     ]
    }
   ],
   "source": [
    "data_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the ID column and reset the indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Won the match #getin . Plus\\u002c tomorrow is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Some areas of New England could see the first ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>@francesco_con40 2nd worst QB. DEFINITELY Tony...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neutral</td>\n",
       "      <td>#Thailand Washington - US President Barack Oba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Did y\\u2019all hear what Tony Romo dressed up ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                           sentence\n",
       "0   neutral  Won the match #getin . Plus\\u002c tomorrow is ...\n",
       "1   neutral  Some areas of New England could see the first ...\n",
       "2  negative  @francesco_con40 2nd worst QB. DEFINITELY Tony...\n",
       "3   neutral  #Thailand Washington - US President Barack Oba...\n",
       "4   neutral  Did y\\u2019all hear what Tony Romo dressed up ..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.drop(\"id\", axis=1, inplace=True)\n",
    "data_df.reset_index(inplace=True, drop=True)\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_irr_char(sentence):\n",
    "    \"\"\"\n",
    "        This function removes irrelevant characters such as punctuations and etc.\n",
    "        Accepts a string sentence\n",
    "        Returns a string cleaned sentence\n",
    "    \"\"\"\n",
    "    word_list = []\n",
    "    tokenized_sent = nltk.word_tokenize(sentence)\n",
    "    for i in tokenized_sent:\n",
    "        word_list.append(''.join(j for j in i if i.isalnum()))\n",
    "\n",
    "    word_list = list(filter(None,  word_list))\n",
    "\n",
    "    return ' '.join(word_list).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentence</th>\n",
       "      <th>cleaned_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Won the match #getin . Plus\\u002c tomorrow is ...</td>\n",
       "      <td>won the match getin tomorrow is a very busy wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Some areas of New England could see the first ...</td>\n",
       "      <td>some areas of new england could see the first ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>@francesco_con40 2nd worst QB. DEFINITELY Tony...</td>\n",
       "      <td>2nd worst qb definitely tony romo the man who ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neutral</td>\n",
       "      <td>#Thailand Washington - US President Barack Oba...</td>\n",
       "      <td>thailand washington us president barack obama ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Did y\\u2019all hear what Tony Romo dressed up ...</td>\n",
       "      <td>did hear what tony romo dressed up as for hall...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                           sentence  \\\n",
       "0   neutral  Won the match #getin . Plus\\u002c tomorrow is ...   \n",
       "1   neutral  Some areas of New England could see the first ...   \n",
       "2  negative  @francesco_con40 2nd worst QB. DEFINITELY Tony...   \n",
       "3   neutral  #Thailand Washington - US President Barack Oba...   \n",
       "4   neutral  Did y\\u2019all hear what Tony Romo dressed up ...   \n",
       "\n",
       "                                    cleaned_sentence  \n",
       "0  won the match getin tomorrow is a very busy wi...  \n",
       "1  some areas of new england could see the first ...  \n",
       "2  2nd worst qb definitely tony romo the man who ...  \n",
       "3  thailand washington us president barack obama ...  \n",
       "4  did hear what tony romo dressed up as for hall...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df[\"cleaned_sentence\"] = data_df.apply(lambda x: remove_irr_char(x[1]), axis=1)\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(sentence):\n",
    "    \"\"\"\n",
    "        This function removes stop words from the cleaned sentence\n",
    "        Accepts a string\n",
    "        Returns a string without the stop words\n",
    "    \"\"\"\n",
    "    tokenized = nltk.word_tokenize(sentence)\n",
    "    stop_words = set({\"a\", \"about\", \"an\", \"are\", \"as\", \"at\", \"be\", \"by\", \"com\", \"de\", \"en\", \"for\", \"from\", \"how\", \"i\", \"in\",\n",
    "                      \"is\", \"it\", \"la\", \"of\", \"on\", \"or\", \"that\", \"this\", \"to\", \"was\", \"what\", \"when\", \"where\", \"who\", \"will\",\n",
    "                      \"with\", \"und\", \"the\", \"www\"})\n",
    "\n",
    "    filtered_list = [\n",
    "        word for word in tokenized if word.casefold() not in stop_words\n",
    "    ]\n",
    "\n",
    "    return ' '.join(filtered_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentence</th>\n",
       "      <th>cleaned_sentence</th>\n",
       "      <th>stopwords_remove_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Won the match #getin . Plus\\u002c tomorrow is ...</td>\n",
       "      <td>won the match getin tomorrow is a very busy wi...</td>\n",
       "      <td>won match getin tomorrow very busy awareness a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Some areas of New England could see the first ...</td>\n",
       "      <td>some areas of new england could see the first ...</td>\n",
       "      <td>some areas new england could see first flakes ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>@francesco_con40 2nd worst QB. DEFINITELY Tony...</td>\n",
       "      <td>2nd worst qb definitely tony romo the man who ...</td>\n",
       "      <td>2nd worst qb definitely tony romo man likes sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neutral</td>\n",
       "      <td>#Thailand Washington - US President Barack Oba...</td>\n",
       "      <td>thailand washington us president barack obama ...</td>\n",
       "      <td>thailand washington us president barack obama ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Did y\\u2019all hear what Tony Romo dressed up ...</td>\n",
       "      <td>did hear what tony romo dressed up as for hall...</td>\n",
       "      <td>did hear tony romo dressed up halloween giants...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                           sentence  \\\n",
       "0   neutral  Won the match #getin . Plus\\u002c tomorrow is ...   \n",
       "1   neutral  Some areas of New England could see the first ...   \n",
       "2  negative  @francesco_con40 2nd worst QB. DEFINITELY Tony...   \n",
       "3   neutral  #Thailand Washington - US President Barack Oba...   \n",
       "4   neutral  Did y\\u2019all hear what Tony Romo dressed up ...   \n",
       "\n",
       "                                    cleaned_sentence  \\\n",
       "0  won the match getin tomorrow is a very busy wi...   \n",
       "1  some areas of new england could see the first ...   \n",
       "2  2nd worst qb definitely tony romo the man who ...   \n",
       "3  thailand washington us president barack obama ...   \n",
       "4  did hear what tony romo dressed up as for hall...   \n",
       "\n",
       "                           stopwords_remove_sentence  \n",
       "0  won match getin tomorrow very busy awareness a...  \n",
       "1  some areas new england could see first flakes ...  \n",
       "2  2nd worst qb definitely tony romo man likes sh...  \n",
       "3  thailand washington us president barack obama ...  \n",
       "4  did hear tony romo dressed up halloween giants...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df[\"stopwords_remove_sentence\"] = data_df.apply(lambda x: remove_stop_words(x[2]), axis=1)\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemmer = PorterStemmer()\n",
    "\n",
    "# def stem_tokens(sentence):\n",
    "#     \"\"\"\n",
    "#         This function stems the sentence to convert each tokens to their root base\n",
    "#         Accepts a string\n",
    "#         Returns a string sentence in their root base\n",
    "#     \"\"\"\n",
    "#     word_list = []\n",
    "#     tokenized_sent = nltk.word_tokenize(sentence)\n",
    "#     for i in tokenized_sent:\n",
    "#         stem_word = stemmer.stem(i)\n",
    "#         word_list.append(stem_word)\n",
    "\n",
    "#     return ' '.join(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_df[\"stemmed_sentence\"] = data_df.apply(lambda x: stem_tokens(x[3]), axis=1)\n",
    "# data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(sentence):\n",
    "    \"\"\"\n",
    "        This function lemmatize the tokens in a sentence to group different variant forms of the same word.\n",
    "        Accepts a string\n",
    "        Returns a string sentence in their lemmatize form\n",
    "    \"\"\"\n",
    "\n",
    "    word_list = []\n",
    "    tokenized_sent = nltk.word_tokenize(sentence)\n",
    "    for i in tokenized_sent:\n",
    "        token = lemmatizer.lemmatize(i)\n",
    "        word_list.append(token)\n",
    "\n",
    "    return ' '.join(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentence</th>\n",
       "      <th>cleaned_sentence</th>\n",
       "      <th>stopwords_remove_sentence</th>\n",
       "      <th>lemmatize_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Won the match #getin . Plus\\u002c tomorrow is ...</td>\n",
       "      <td>won the match getin tomorrow is a very busy wi...</td>\n",
       "      <td>won match getin tomorrow very busy awareness a...</td>\n",
       "      <td>won match getin tomorrow very busy awareness a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Some areas of New England could see the first ...</td>\n",
       "      <td>some areas of new england could see the first ...</td>\n",
       "      <td>some areas new england could see first flakes ...</td>\n",
       "      <td>some area new england could see first flake se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>@francesco_con40 2nd worst QB. DEFINITELY Tony...</td>\n",
       "      <td>2nd worst qb definitely tony romo the man who ...</td>\n",
       "      <td>2nd worst qb definitely tony romo man likes sh...</td>\n",
       "      <td>2nd worst qb definitely tony romo man like sha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neutral</td>\n",
       "      <td>#Thailand Washington - US President Barack Oba...</td>\n",
       "      <td>thailand washington us president barack obama ...</td>\n",
       "      <td>thailand washington us president barack obama ...</td>\n",
       "      <td>thailand washington u president barack obama v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Did y\\u2019all hear what Tony Romo dressed up ...</td>\n",
       "      <td>did hear what tony romo dressed up as for hall...</td>\n",
       "      <td>did hear tony romo dressed up halloween giants...</td>\n",
       "      <td>did hear tony romo dressed up halloween giant ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                           sentence  \\\n",
       "0   neutral  Won the match #getin . Plus\\u002c tomorrow is ...   \n",
       "1   neutral  Some areas of New England could see the first ...   \n",
       "2  negative  @francesco_con40 2nd worst QB. DEFINITELY Tony...   \n",
       "3   neutral  #Thailand Washington - US President Barack Oba...   \n",
       "4   neutral  Did y\\u2019all hear what Tony Romo dressed up ...   \n",
       "\n",
       "                                    cleaned_sentence  \\\n",
       "0  won the match getin tomorrow is a very busy wi...   \n",
       "1  some areas of new england could see the first ...   \n",
       "2  2nd worst qb definitely tony romo the man who ...   \n",
       "3  thailand washington us president barack obama ...   \n",
       "4  did hear what tony romo dressed up as for hall...   \n",
       "\n",
       "                           stopwords_remove_sentence  \\\n",
       "0  won match getin tomorrow very busy awareness a...   \n",
       "1  some areas new england could see first flakes ...   \n",
       "2  2nd worst qb definitely tony romo man likes sh...   \n",
       "3  thailand washington us president barack obama ...   \n",
       "4  did hear tony romo dressed up halloween giants...   \n",
       "\n",
       "                                  lemmatize_sentence  \n",
       "0  won match getin tomorrow very busy awareness a...  \n",
       "1  some area new england could see first flake se...  \n",
       "2  2nd worst qb definitely tony romo man like sha...  \n",
       "3  thailand washington u president barack obama v...  \n",
       "4  did hear tony romo dressed up halloween giant ...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df[\"lemmatize_sentence\"] = data_df.apply(lambda x: lemmatize(x[3]), axis=1)\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check for the value distribution of the sentiment in our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "neutral     6838\n",
       "positive    5690\n",
       "negative    2357\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df[\"sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us convert the sentiment column to numerical (1 = Positive, 0 = Neutral, -1 = Negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df[\"sentiment\"] = np.where((data_df[\"sentiment\"] == \"positive\"), +1, data_df[\"sentiment\"])\n",
    "data_df[\"sentiment\"] = np.where((data_df[\"sentiment\"] == \"negative\"), -1, data_df[\"sentiment\"])\n",
    "data_df[\"sentiment\"] = np.where((data_df[\"sentiment\"] == \"neutral\"), 0, data_df[\"sentiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "0     6838\n",
       "1     5690\n",
       "-1    2357\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df[\"sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentence</th>\n",
       "      <th>cleaned_sentence</th>\n",
       "      <th>stopwords_remove_sentence</th>\n",
       "      <th>lemmatize_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Won the match #getin . Plus\\u002c tomorrow is ...</td>\n",
       "      <td>won the match getin tomorrow is a very busy wi...</td>\n",
       "      <td>won match getin tomorrow very busy awareness a...</td>\n",
       "      <td>won match getin tomorrow very busy awareness a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Some areas of New England could see the first ...</td>\n",
       "      <td>some areas of new england could see the first ...</td>\n",
       "      <td>some areas new england could see first flakes ...</td>\n",
       "      <td>some area new england could see first flake se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "      <td>@francesco_con40 2nd worst QB. DEFINITELY Tony...</td>\n",
       "      <td>2nd worst qb definitely tony romo the man who ...</td>\n",
       "      <td>2nd worst qb definitely tony romo man likes sh...</td>\n",
       "      <td>2nd worst qb definitely tony romo man like sha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>#Thailand Washington - US President Barack Oba...</td>\n",
       "      <td>thailand washington us president barack obama ...</td>\n",
       "      <td>thailand washington us president barack obama ...</td>\n",
       "      <td>thailand washington u president barack obama v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Did y\\u2019all hear what Tony Romo dressed up ...</td>\n",
       "      <td>did hear what tony romo dressed up as for hall...</td>\n",
       "      <td>did hear tony romo dressed up halloween giants...</td>\n",
       "      <td>did hear tony romo dressed up halloween giant ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                           sentence  \\\n",
       "0         0  Won the match #getin . Plus\\u002c tomorrow is ...   \n",
       "1         0  Some areas of New England could see the first ...   \n",
       "2        -1  @francesco_con40 2nd worst QB. DEFINITELY Tony...   \n",
       "3         0  #Thailand Washington - US President Barack Oba...   \n",
       "4         0  Did y\\u2019all hear what Tony Romo dressed up ...   \n",
       "\n",
       "                                    cleaned_sentence  \\\n",
       "0  won the match getin tomorrow is a very busy wi...   \n",
       "1  some areas of new england could see the first ...   \n",
       "2  2nd worst qb definitely tony romo the man who ...   \n",
       "3  thailand washington us president barack obama ...   \n",
       "4  did hear what tony romo dressed up as for hall...   \n",
       "\n",
       "                           stopwords_remove_sentence  \\\n",
       "0  won match getin tomorrow very busy awareness a...   \n",
       "1  some areas new england could see first flakes ...   \n",
       "2  2nd worst qb definitely tony romo man likes sh...   \n",
       "3  thailand washington us president barack obama ...   \n",
       "4  did hear tony romo dressed up halloween giants...   \n",
       "\n",
       "                                  lemmatize_sentence  \n",
       "0  won match getin tomorrow very busy awareness a...  \n",
       "1  some area new england could see first flake se...  \n",
       "2  2nd worst qb definitely tony romo man like sha...  \n",
       "3  thailand washington u president barack obama v...  \n",
       "4  did hear tony romo dressed up halloween giant ...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Model without feature engineering using  CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>cleaned_sentence</th>\n",
       "      <th>stopwords_remove_sentence</th>\n",
       "      <th>lemmatize_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>won the match getin tomorrow is a very busy wi...</td>\n",
       "      <td>won match getin tomorrow very busy awareness a...</td>\n",
       "      <td>won match getin tomorrow very busy awareness a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>some areas of new england could see the first ...</td>\n",
       "      <td>some areas new england could see first flakes ...</td>\n",
       "      <td>some area new england could see first flake se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "      <td>2nd worst qb definitely tony romo the man who ...</td>\n",
       "      <td>2nd worst qb definitely tony romo man likes sh...</td>\n",
       "      <td>2nd worst qb definitely tony romo man like sha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>thailand washington us president barack obama ...</td>\n",
       "      <td>thailand washington us president barack obama ...</td>\n",
       "      <td>thailand washington u president barack obama v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>did hear what tony romo dressed up as for hall...</td>\n",
       "      <td>did hear tony romo dressed up halloween giants...</td>\n",
       "      <td>did hear tony romo dressed up halloween giant ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                   cleaned_sentence  \\\n",
       "0         0  won the match getin tomorrow is a very busy wi...   \n",
       "1         0  some areas of new england could see the first ...   \n",
       "2        -1  2nd worst qb definitely tony romo the man who ...   \n",
       "3         0  thailand washington us president barack obama ...   \n",
       "4         0  did hear what tony romo dressed up as for hall...   \n",
       "\n",
       "                           stopwords_remove_sentence  \\\n",
       "0  won match getin tomorrow very busy awareness a...   \n",
       "1  some areas new england could see first flakes ...   \n",
       "2  2nd worst qb definitely tony romo man likes sh...   \n",
       "3  thailand washington us president barack obama ...   \n",
       "4  did hear tony romo dressed up halloween giants...   \n",
       "\n",
       "                                  lemmatize_sentence  \n",
       "0  won match getin tomorrow very busy awareness a...  \n",
       "1  some area new england could see first flake se...  \n",
       "2  2nd worst qb definitely tony romo man like sha...  \n",
       "3  thailand washington u president barack obama v...  \n",
       "4  did hear tony romo dressed up halloween giant ...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_mod = data_df[[\"sentiment\", \"cleaned_sentence\", \"stopwords_remove_sentence\", \"lemmatize_sentence\"]]\n",
    "data_mod.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a bag of words using count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_count = CountVectorizer(\n",
    "    min_df = 3,\n",
    "    ngram_range = (1, 1)\n",
    ")\n",
    "\n",
    "vectorizer_count2 = CountVectorizer(\n",
    "    min_df = 3,\n",
    "    ngram_range = (1, 1)\n",
    ")\n",
    "\n",
    "vectorizer_count3 = CountVectorizer(\n",
    "    min_df = 3,\n",
    "    ngram_range = (1, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_lemma = vectorizer_count.fit_transform(\n",
    "    data_mod[\"lemmatize_sentence\"]\n",
    ")\n",
    "\n",
    "features_lemma_nd = features_lemma.toarray()\n",
    "\n",
    "features_stopwords = vectorizer_count2.fit_transform(\n",
    "    data_mod[\"stopwords_remove_sentence\"]\n",
    ")\n",
    "\n",
    "features_stopwords_nd = features_stopwords.toarray()\n",
    "\n",
    "features_cleaned = vectorizer_count3.fit_transform(\n",
    "    data_mod[\"cleaned_sentence\"]\n",
    ")\n",
    "\n",
    "features_cleaned_nd = features_cleaned.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_labels = []\n",
    "for i in data_mod[\"sentiment\"]:\n",
    "    data_labels.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X_train_stemmed_cv, X_test_stemmed_cv, y_train_stemmed_cv, y_test_stemmed_cv  = train_test_split(\n",
    "#         features_stemmed_nd, \n",
    "#         data_labels,\n",
    "#         train_size=0.80, \n",
    "#         random_state=10)\n",
    "\n",
    "X_train_lemma_cv, X_test_lemma_cv, y_train_lemma_cv, y_test_lemma_cv = train_test_split(\n",
    "    features_lemma_nd,\n",
    "    data_labels,\n",
    "    train_size=0.80,\n",
    "    random_state=10\n",
    ")\n",
    "\n",
    "X_train_stopwords_cv, X_test_stopwords_cv, y_train_stopwords_cv, y_test_stopwords_cv = train_test_split(\n",
    "    features_stopwords_nd,\n",
    "    data_labels,\n",
    "    train_size=0.80,\n",
    "    random_state=10\n",
    ")\n",
    "\n",
    "X_train_cleaned_cv, X_test_cleaned_cv, y_train_cleaned_cv, y_test_cleaned_cv = train_test_split(\n",
    "    features_cleaned_nd,\n",
    "    data_labels,\n",
    "    train_size=0.80,\n",
    "    random_state=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_model_1 = LogisticRegression(max_iter=1000)\n",
    "log_model_2 = LogisticRegression(max_iter=1000)\n",
    "log_model_3 = LogisticRegression(max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_model_1 = log_model_1.fit(X_train_lemma_cv, y_train_lemma_cv)\n",
    "log_model_2 = log_model_2.fit(X_train_stopwords_cv, y_train_stopwords_cv)\n",
    "log_model_3 = log_model_3.fit(X_train_cleaned_cv, y_train_cleaned_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_cv1 = log_model_1.predict(X_test_lemma_cv)\n",
    "y_pred_cv2 = log_model_2.predict(X_test_stopwords_cv)\n",
    "y_pred_cv3 = log_model_3.predict(X_test_cleaned_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.663419549882432\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.57      0.42      0.48       483\n",
      "           0       0.67      0.74      0.71      1361\n",
      "           1       0.68      0.67      0.68      1133\n",
      "\n",
      "    accuracy                           0.66      2977\n",
      "   macro avg       0.64      0.61      0.62      2977\n",
      "weighted avg       0.66      0.66      0.66      2977\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model 1\n",
    "print(accuracy_score(y_test_lemma_cv, y_pred_cv1))\n",
    "print(classification_report(y_test_lemma_cv, y_pred_cv1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6627477326167283\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.57      0.42      0.49       483\n",
      "           0       0.67      0.75      0.71      1361\n",
      "           1       0.69      0.66      0.67      1133\n",
      "\n",
      "    accuracy                           0.66      2977\n",
      "   macro avg       0.64      0.61      0.62      2977\n",
      "weighted avg       0.66      0.66      0.66      2977\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model 2\n",
    "print(accuracy_score(y_test_stopwords_cv, y_pred_cv2))\n",
    "print(classification_report(y_test_stopwords_cv, y_pred_cv2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6677863621095063\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.60      0.43      0.50       483\n",
      "           0       0.67      0.75      0.71      1361\n",
      "           1       0.69      0.67      0.68      1133\n",
      "\n",
      "    accuracy                           0.67      2977\n",
      "   macro avg       0.65      0.62      0.63      2977\n",
      "weighted avg       0.66      0.67      0.66      2977\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model 3\n",
    "print(accuracy_score(y_test_cleaned_cv, y_pred_cv3))\n",
    "print(classification_report(y_test_cleaned_cv, y_pred_cv3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Model without feature engineering using TFIDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_tfidf = TfidfVectorizer(min_df=3)\n",
    "\n",
    "vectorizer_tfidf2 = TfidfVectorizer(min_df=3)\n",
    "\n",
    "vectorizer_tfidf3 = TfidfVectorizer(min_df=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_lemma_tfidf = vectorizer_tfidf.fit_transform(data_mod[\"lemmatize_sentence\"])\n",
    "\n",
    "features_lemma_tfidf_nd = features_lemma_tfidf.toarray()\n",
    "\n",
    "features_stopwords_tfidf = vectorizer_tfidf2.fit_transform(data_mod[\"stopwords_remove_sentence\"])\n",
    "\n",
    "features_stopwords_tfidf_nd = features_stopwords_tfidf.toarray()\n",
    "\n",
    "features_cleaned_tfidf = vectorizer_tfidf3.fit_transform(data_mod[\"cleaned_sentence\"])\n",
    "\n",
    "features_cleaned_tfidf_nd = features_cleaned_tfidf.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lemma_tfidf, X_test_lemma_tfidf, y_train_lemma_tfidf, y_test_lemma_tfidf  = train_test_split(\n",
    "        features_lemma_tfidf_nd, \n",
    "        data_labels,\n",
    "        train_size=0.80, \n",
    "        random_state=10)\n",
    "\n",
    "X_train_stopwords_tfidf, X_test_stopwords_tfidf, y_train_stopwords_tfidf, y_test_stopwords_tfidf  = train_test_split(\n",
    "        features_stopwords_tfidf_nd, \n",
    "        data_labels,\n",
    "        train_size=0.80, \n",
    "        random_state=10)\n",
    "\n",
    "X_train_cleaned_tfidf, X_test_cleaned_tfidf, y_train_cleaned_tfidf, y_test_cleaned_tfidf  = train_test_split(\n",
    "        features_cleaned_tfidf_nd, \n",
    "        data_labels,\n",
    "        train_size=0.80, \n",
    "        random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_model_4 =  LogisticRegression(max_iter=1000)\n",
    "log_model_5 =  LogisticRegression(max_iter=1000)\n",
    "log_model_6 =  LogisticRegression(max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_model_4 = log_model_4.fit(X_train_lemma_tfidf, y_train_lemma_tfidf)\n",
    "log_model_5 = log_model_5.fit(X_train_stopwords_tfidf, y_train_stopwords_tfidf)\n",
    "log_model_6 = log_model_6.fit(X_train_cleaned_tfidf, y_train_cleaned_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6711454484380248\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.69      0.30      0.42       483\n",
      "           0       0.65      0.82      0.72      1361\n",
      "           1       0.71      0.65      0.68      1133\n",
      "\n",
      "    accuracy                           0.67      2977\n",
      "   macro avg       0.68      0.59      0.61      2977\n",
      "weighted avg       0.68      0.67      0.66      2977\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_stemmed_tfidf = log_model_4.predict(X_test_lemma_tfidf)\n",
    "print(accuracy_score(y_test_lemma_tfidf, y_pred_stemmed_tfidf))\n",
    "print(classification_report(y_test_lemma_tfidf, y_pred_stemmed_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6741686261336917\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.70      0.30      0.42       483\n",
      "           0       0.65      0.83      0.73      1361\n",
      "           1       0.72      0.65      0.68      1133\n",
      "\n",
      "    accuracy                           0.67      2977\n",
      "   macro avg       0.69      0.59      0.61      2977\n",
      "weighted avg       0.68      0.67      0.66      2977\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_stopwords_tfidf = log_model_5.predict(X_test_stopwords_tfidf)\n",
    "print(accuracy_score(y_test_stopwords_tfidf, y_pred_stopwords_tfidf))\n",
    "print(classification_report(y_test_stopwords_tfidf, y_pred_stopwords_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6741686261336917\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.69      0.31      0.43       483\n",
      "           0       0.65      0.82      0.73      1361\n",
      "           1       0.71      0.65      0.68      1133\n",
      "\n",
      "    accuracy                           0.67      2977\n",
      "   macro avg       0.68      0.60      0.61      2977\n",
      "weighted avg       0.68      0.67      0.66      2977\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_cleaned_tfidf = log_model_6.predict(X_test_cleaned_tfidf)\n",
    "print(accuracy_score(y_test_cleaned_tfidf, y_pred_cleaned_tfidf))\n",
    "print(classification_report(y_test_cleaned_tfidf, y_pred_cleaned_tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Model without Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB(force_alpha=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB(force_alpha=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB(force_alpha=True)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multinomialNB = MultinomialNB(force_alpha=True)\n",
    "\n",
    "multinomialNB.fit(X_train_stopwords_cv, y_train_stopwords_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6341954988243198\n"
     ]
    }
   ],
   "source": [
    "y_pred_nb_cv = multinomialNB.predict(X_test_stopwords_cv)\n",
    "print(accuracy_score(y_test_stopwords_cv, y_pred_nb_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB(force_alpha=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB(force_alpha=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB(force_alpha=True)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multinomialNB2 = MultinomialNB(force_alpha=True)\n",
    "\n",
    "multinomialNB2.fit(X_train_lemma_cv, y_train_lemma_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6348673160900236\n"
     ]
    }
   ],
   "source": [
    "y_pred_nb_cv2 = multinomialNB2.predict(X_test_lemma_cv)\n",
    "print(accuracy_score(y_test_lemma_cv, y_pred_nb_cv2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6167282499160228\n"
     ]
    }
   ],
   "source": [
    "multinomialNB3 = MultinomialNB(force_alpha=True)\n",
    "\n",
    "multinomialNB3.fit(X_train_stopwords_tfidf, y_train_stopwords_tfidf)\n",
    "\n",
    "y_pred_nb_tfidf = multinomialNB3.predict(X_test_stopwords_tfidf)\n",
    "print(accuracy_score(y_test_stopwords_tfidf, y_pred_nb_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6167282499160228\n"
     ]
    }
   ],
   "source": [
    "multinomialNB4 = MultinomialNB(force_alpha=True)\n",
    "\n",
    "multinomialNB4.fit(X_train_lemma_tfidf, y_train_lemma_tfidf)\n",
    "\n",
    "y_pred_nb_tfidf2 = multinomialNB4.predict(X_test_lemma_tfidf)\n",
    "print(accuracy_score(y_test_lemma_tfidf, y_pred_nb_tfidf2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Models with Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 6838, -1: 6838, 1: 6838})\n"
     ]
    }
   ],
   "source": [
    "oversample = RandomOverSampler(sampling_strategy='minority')\n",
    "\n",
    "X_over, y_over = oversample.fit_resample(features_stopwords_nd, data_labels)\n",
    "X_over, y_over = oversample.fit_resample(X_over, y_over)\n",
    "print(Counter(y_over))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_os, X_test_os, y_train_os, y_test_os  = train_test_split(\n",
    "        X_over, \n",
    "        y_over,\n",
    "        train_size=0.80, \n",
    "        random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_model_7 = LogisticRegression(max_iter=1000)\n",
    "log_model_7 = log_model_7.fit(X_train_os, y_train_os)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7735803070923715\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.82      0.91      0.86      1411\n",
      "           0       0.72      0.68      0.70      1318\n",
      "           1       0.77      0.72      0.75      1374\n",
      "\n",
      "    accuracy                           0.77      4103\n",
      "   macro avg       0.77      0.77      0.77      4103\n",
      "weighted avg       0.77      0.77      0.77      4103\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = log_model_7.predict(X_test_os)\n",
    "print(accuracy_score(y_test_os, y_pred))\n",
    "print(classification_report(y_test_os, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 6838, -1: 6838, 1: 6838})\n"
     ]
    }
   ],
   "source": [
    "oversample2 = RandomOverSampler(sampling_strategy='minority')\n",
    "\n",
    "X_over2, y_over2 = oversample2.fit_resample(features_lemma_nd, data_labels)\n",
    "X_over2, y_over2 = oversample2.fit_resample(X_over2, y_over2)\n",
    "print(Counter(y_over2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_os2, X_test_os2, y_train_os2, y_test_os2  = train_test_split(\n",
    "        X_over2, \n",
    "        y_over2,\n",
    "        train_size=0.80, \n",
    "        random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_model_8 = LogisticRegression(max_iter=1000)\n",
    "log_model_8 = log_model_8.fit(X_train_os2, y_train_os2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7701681696319767\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.81      0.90      0.85      1411\n",
      "           0       0.70      0.69      0.70      1318\n",
      "           1       0.79      0.72      0.75      1374\n",
      "\n",
      "    accuracy                           0.77      4103\n",
      "   macro avg       0.77      0.77      0.77      4103\n",
      "weighted avg       0.77      0.77      0.77      4103\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred2 = log_model_8.predict(X_test_os2)\n",
    "print(accuracy_score(y_test_os2, y_pred2))\n",
    "print(classification_report(y_test_os2, y_pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets apply GridSearch to get the best hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# parameters = {\n",
    "#     'C': np.logspace(-5, 8, 15),\n",
    "#     'max_iter': [1,2,3,4,5,10,20,30,40,50,60,70,80,90,100,110,120,130,140,150]\n",
    "# }\n",
    "\n",
    "# grid_search = LogisticRegression()\n",
    "\n",
    "# grid_search = GridSearchCV(\n",
    "#     grid_search,\n",
    "#     parameters,\n",
    "#     cv = 5,\n",
    "#     scoring = 'accuracy',\n",
    "#     n_jobs=-1\n",
    "# )\n",
    "\n",
    "# grid_result = grid_search.fit(X_train, y_train)\n",
    "\n",
    "# print(\"Best parameters: \", grid_result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_model_8 = LogisticRegression(C = 3.727593720314938, max_iter=40)\n",
    "# log_model_8.fit(X_train_os, y_train_os)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = log_model_8.predict(X_test_os)\n",
    "# print(accuracy_score(y_test_os, y_pred))\n",
    "# print(classification_report(y_test_os, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes with Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB(force_alpha=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB(force_alpha=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB(force_alpha=True)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multinomialNB_os = MultinomialNB(force_alpha=True)\n",
    "\n",
    "multinomialNB_os.fit(X_train_os, y_train_os)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7019254204240799\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.74      0.82      0.78      1411\n",
      "           0       0.67      0.58      0.62      1318\n",
      "           1       0.69      0.70      0.69      1374\n",
      "\n",
      "    accuracy                           0.70      4103\n",
      "   macro avg       0.70      0.70      0.70      4103\n",
      "weighted avg       0.70      0.70      0.70      4103\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_nb_os = multinomialNB_os.predict(X_test_os)\n",
    "print(accuracy_score(y_test_os, y_pred_nb_os))\n",
    "print(classification_report(y_test_os, y_pred_nb_os))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB(force_alpha=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB(force_alpha=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB(force_alpha=True)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multinomialNB_os2 = MultinomialNB(force_alpha=True)\n",
    "\n",
    "multinomialNB_os2.fit(X_train_os2, y_train_os2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7026565927370217\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.74      0.82      0.78      1411\n",
      "           0       0.68      0.59      0.63      1318\n",
      "           1       0.69      0.70      0.69      1374\n",
      "\n",
      "    accuracy                           0.70      4103\n",
      "   macro avg       0.70      0.70      0.70      4103\n",
      "weighted avg       0.70      0.70      0.70      4103\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_nb_os2 = multinomialNB_os2.predict(X_test_os2)\n",
    "print(accuracy_score(y_test_os2, y_pred_nb_os2))\n",
    "print(classification_report(y_test_os2, y_pred_nb_os2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models using cleaned sentence and nltk.stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "vectorizer_tfidf = TfidfVectorizer(min_df=5, max_df=0.8, max_features=2500, stop_words = stopwords.words('english'))\n",
    "\n",
    "features_cleaned_tfidf_2 = vectorizer_tfidf.fit_transform(data_mod[\"cleaned_sentence\"])\n",
    "\n",
    "features_cleaned_tfidf_2_nd = features_cleaned_tfidf_2.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 6838, -1: 6838, 1: 6838})\n"
     ]
    }
   ],
   "source": [
    "X_over, y_over = oversample.fit_resample(features_cleaned_tfidf_2_nd, data_labels)\n",
    "X_over, y_over = oversample.fit_resample(X_over, y_over)\n",
    "print(Counter(y_over))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cleaned_tfidf_2, X_test_cleaned_tfidf_2, y_train_cleaned_tfidf_2, y_test_cleaned_tfidf_2  = train_test_split(\n",
    "        X_over, \n",
    "        y_over,\n",
    "        train_size=0.80, \n",
    "        random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_model_9 = LogisticRegression(max_iter=1000)\n",
    "log_model_9 = log_model_9.fit(X_train_cleaned_tfidf_2, y_train_cleaned_tfidf_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6931513526687789\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.73      0.76      0.74      1411\n",
      "           0       0.62      0.68      0.65      1318\n",
      "           1       0.74      0.64      0.69      1374\n",
      "\n",
      "    accuracy                           0.69      4103\n",
      "   macro avg       0.70      0.69      0.69      4103\n",
      "weighted avg       0.70      0.69      0.69      4103\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = log_model_9.predict(X_test_cleaned_tfidf_2)\n",
    "print(accuracy_score(y_test_cleaned_tfidf_2, y_pred))\n",
    "print(classification_report(y_test_cleaned_tfidf_2, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving models to a File Format using JobLib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename = \"log_reg_oversampling_stopwords_78acc.joblib\"\n",
    "vectorizer_filename = \"count_vectorizer.joblib\"\n",
    "joblib.dump(log_model_7, \"nlp_project/models/\" + model_filename)\n",
    "joblib.dump(vectorizer_count2, \"nlp_project/models/\" + vectorizer_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
